# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Run inference on images, videos, directories, streams, etc.

Usage - sources:
    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam
                                                             img.jpg        # image
                                                             vid.mp4        # video
                                                             path/          # directory
                                                             path/*.jpg     # glob
                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch
                                         yolov5s.torchscript        # TorchScript
                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                         yolov5s.xml                # OpenVINO
                                         yolov5s.engine             # TensorRT
                                         yolov5s.mlmodel            # CoreML (macOS-only)
                                         yolov5s_saved_model        # TensorFlow SavedModel
                                         yolov5s.pb                 # TensorFlow GraphDef
                                         yolov5s.tflite             # TensorFlow Lite
                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
"""

# å¯¼å…¥éœ€è¦çš„åº“
import os
import sys
from pathlib import Path
import numpy as np
import cv2
import torch
import torch.backends.cudnn as cudnn

# åˆå§‹åŒ–ç›®å½•
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # å®šä¹‰YOLOv5çš„æ ¹ç›®å½•
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # å°†YOLOv5çš„æ ¹ç›®å½•æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ä¸­ï¼ˆç¨‹åºç»“æŸååˆ é™¤ï¼‰
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams
from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,
                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, time_sync

# å¯¼å…¥letterbox
from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective

weights = ROOT / "best.pt"  # æƒé‡æ–‡ä»¶åœ°å€   .ptæ–‡ä»¶
source = ROOT / "data/images"  # æµ‹è¯•æ•°æ®æ–‡ä»¶(å›¾ç‰‡æˆ–è§†é¢‘)çš„ä¿å­˜è·¯å¾„
data = ROOT / "data/VOC.yaml"  # æ ‡ç­¾æ–‡ä»¶åœ°å€   .yamlæ–‡ä»¶

imgsz = (320, 320)  # è¾“å…¥å›¾ç‰‡çš„å¤§å° é»˜è®¤640(pixels)
conf_thres = 0.20  # objectç½®ä¿¡åº¦é˜ˆå€¼ é»˜è®¤0.25  ç”¨åœ¨nmsä¸­
iou_thres = 0.45  # åšnmsçš„ioué˜ˆå€¼ é»˜è®¤0.45   ç”¨åœ¨nmsä¸­
max_det = 5  # æ¯å¼ å›¾ç‰‡æœ€å¤šçš„ç›®æ ‡æ•°é‡  ç”¨åœ¨nmsä¸­
device = ""  # è®¾ç½®ä»£ç æ‰§è¡Œçš„è®¾å¤‡ cuda device, i.e. 0 or 0,1,2,3 or cpu
classes = None  # åœ¨nmsä¸­æ˜¯å¦æ˜¯åªä¿ç•™æŸäº›ç‰¹å®šçš„ç±» é»˜è®¤æ˜¯None å°±æ˜¯æ‰€æœ‰ç±»åªè¦æ»¡è¶³æ¡ä»¶éƒ½å¯ä»¥ä¿ç•™ --class 0, or --class 0 2 3
agnostic_nms = False  # è¿›è¡Œnmsæ˜¯å¦ä¹Ÿé™¤å»ä¸åŒç±»åˆ«ä¹‹é—´çš„æ¡† é»˜è®¤False
augment = False  # é¢„æµ‹æ˜¯å¦ä¹Ÿè¦é‡‡ç”¨æ•°æ®å¢å¼º TTA é»˜è®¤False
visualize = False  # ç‰¹å¾å›¾å¯è§†åŒ– é»˜è®¤FALSE
half = False  # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦ Float16 æ¨ç† å¯ä»¥ç¼©çŸ­æ¨ç†æ—¶é—´ ä½†æ˜¯é»˜è®¤æ˜¯False
dnn = False  # ä½¿ç”¨OpenCV DNNè¿›è¡ŒONNXæ¨ç†

# è·å–è®¾å¤‡
device = select_device(device)

# è½½å…¥æ¨¡å‹
model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)
stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine
imgsz = check_img_size(imgsz, s=stride)  # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸

# Half
# ä½¿ç”¨åŠç²¾åº¦ Float16 æ¨ç†
half &= (pt or jit or onnx or engine) and device.type != "cpu"  # FP16 supported on limited backends with CUDA
if pt or jit:
    model.model.half() if half else model.model.float()


def detect(img):
    # Dataloader
    # è½½å…¥æ•°æ®
    # dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)
    # Run inference
    # å¼€å§‹é¢„æµ‹
    # model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup
    model.warmup(imgsz=(1, 3, *imgsz))  # warmup
    dt, seen = [0.0, 0.0, 0.0], 0
    # å¯¹å›¾ç‰‡è¿›è¡Œå¤„ç†
    im0 = img
    # Padded resize
    im = letterbox(im0, imgsz, stride, auto=pt)[0]
    # Convert
    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
    im = np.ascontiguousarray(im)
    t1 = time_sync()
    im = torch.from_numpy(im).to(device)
    im = im.half() if half else im.float()  # uint8 to fp16/32
    im /= 255  # 0 - 255 to 0.0 - 1.0
    if len(im.shape) == 3:
        im = im[None]  # expand for batch dim
    t2 = time_sync()
    dt[0] += t2 - t1
    # Inference
    # é¢„æµ‹
    pred = model(im, augment=augment, visualize=visualize)
    t3 = time_sync()
    dt[1] += t3 - t2
    # NMS
    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
    dt[2] += time_sync() - t3
    # ç”¨äºå­˜æ”¾ç»“æœ
    detections = []
    # Process predictions
    for i, det in enumerate(pred):  # per image æ¯å¼ å›¾ç‰‡
        seen += 1
        # im0 = im0s.copy()
        if len(det):
            # Rescale boxes from img_size to im0 size
            det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()
            # Write results
            # å†™å…¥ç»“æœ
            for *xyxy, conf, cls in reversed(det):
                xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4))).view(-1).tolist()
                xywh = [round(x) for x in xywh]
                xywh = [xywh[0] - xywh[2] // 2, xywh[1] - xywh[3] // 2, xywh[2],
                        xywh[3]]  # æ£€æµ‹åˆ°ç›®æ ‡ä½ç½®ï¼Œæ ¼å¼ï¼šï¼ˆleftï¼Œtopï¼Œwï¼Œhï¼‰
                cls = names[int(cls)]
                conf = float(conf)
                detections.append({"class": cls, "conf": conf, "position": xywh})
    # è¾“å‡ºç»“æœi
    # for i in detections:
    #     print(i)
    # æ¨æµ‹çš„æ—¶é—´
    # LOGGER.info(f"({t3 - t2:.3f}s)")
    return detections
